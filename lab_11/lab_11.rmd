---
title: "lab_11"
author: "derek willis"
date: "2023-04-26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## You will need

* Our usual libraries for working with data, plus rvest.


## Load libraries and establish settings

**Task** Create a codeblock and load appropriate packages and settings for this lab.

```{r}

library(rvest)
library(tidyverse)
library(janitor)
library(lubridate)
library(dplyr)
library(stringr)
```


Let's get to scraping.

## Questions

**Q1**. Scrape the listing of available Maryland state grants at https://grants.maryland.gov/Pages/StateGrants.aspx into a dataframe. You should have three columns, one of which is a date, so make sure the date column has a date datatype. Then, write code to count the number of grants opportunities offered by each organization listed in your dataframe, showing the organization with the most grant opportunities first. Which state agency has the most?

**A1** Maryland Energy Administration

```{r}

grants_url <- "https://grants.maryland.gov/Pages/StateGrants.aspx" 
   


# show the dataframe

grants_url
```

```{r}

grants <- grants_url |>
 read_html() |>
   html_table() |>
  bind_rows() |>
  clean_names()

# show the dataframe

grants
```


```{r}
grants_withnames <- grants |>
   clean_names() 


grants_withnames
```

```{r}
 grants_withdates <- grants_withnames |>
  mutate(due_date = as.Date(due_date))

grants_withdates


```

```{r}
grants_filtered <- grants_withdates |>
  group_by(organization) |>
  count() |>
  arrange(desc(n))


grants_filtered
```


# display the html below



**Q2** Next, let's scrape the list of press releases from Maryland's Office of the Public Defender, https://www.opd.state.md.us/press-releases. This isn't a table, so you'll need to use `html_elements()` and your browser's inspector and do some clean up on the results. The result should be a dataframe with two columns that contain the date and title, and the date column should have a date datatype. The challenge here is figuring out how to isolate the releases.

When you finish scraping into a dataframe, write code to find the press releases that have the word "police" in the title. How many are there and when was the most recent one?

**A2** There are 8 press releases with police in the title; the most recent was 2021-06-21.

```{r}
public_defender_url <- "https://www.opd.state.md.us/press-releases"

public_defender <- public_defender_url |>
  read_html() |>
  html_elements('span.wixui-rich-text__text') |>
  html_text() |>
  as_tibble()

public_defender


```
```{r}
#chat gpt helped code of cutting top blank lines
public_defender_cut <- public_defender |>
  slice(-1:-9)

#split column 
public_defender_split <- public_defender_cut |> 
  separate(value, into = c("Date", "Text"), sep = ": ", extra = "merge") |>
  mutate(Date = mdy(Date)) 

# View 
print(public_defender_split)

```
```{r}
#remove blank rows with chat gpt help
public_defender_releases <- public_defender_split |>
  remove_empty() |>
  distinct()
```

```{r}
# Check the column names in your DataFrame
colnames(public_defender_releases)



public_defender_releases <- public_defender_releases |> 
  rename(release = Text) 
  

```


```{r}
public_defender_releases <- public_defender_releases |>
  clean_names()
```


```{r}
# Remove rows where the 'releases' column is NA
public_defender_releases_filtered <- public_defender_releases[!is.na(public_defender_releases$releases), ]

# View the modified data frame
print(public_defender_releases_filtered)


```
```{r}
# Check the structure of the DataFrame
str(public_defender_releases)

# Check the number of rows in the DataFrame
nrow(public_defender_releases)

# Check the column names
colnames(public_defender_releases)

```
```{r}
# Remove rows where the 'release' column is NA
public_defender_releases_filtered <- public_defender_releases[!is.na(public_defender_releases$release), ]

# View the modified data frame
print(public_defender_releases_filtered)

```

```{r}


# filter police
police_releases <- public_defender_releases_filtered |>
  filter(str_detect(release, "Police")) |>
  arrange(desc(date))




```


**Q3** Sen. Ben Cardin, D-Maryland, has posted hundreds of press releases at https://www.cardin.senate.gov/?post_type=press-releases. It would be great to have all of them in a dataframe that has the following columns: date, title and url.

To do this, you will need to scrape the page's html and save that to a variable, and _then_ extract the dates, titles and urls into _separate_ dataframes using html_elements(). And remember how we turn a list into a dataframe. The function `html_text()` pulls out the contents of a tag, but for urls we want the HTML attribute. Rvest gives you a way to extract the URL from a link; google to find out what it is.

At the end, you'll have three dataframes that you want to combine into a single dataframe. When we want to combine the rows of identical dataframes, we used `bind_rows()`. If you were combining columns instead of rows, there's a similar function. Find out what it is and use it to put all of the dataframes together into a single one.

When you're done, rename the columns so they make sense, then make sure the date column is an actual date.

Finally, tell me what questions you could ask of this data. Be creative.

**A3** 
How often does Cardin issue releases with other members of the Maryland delegation?
How often does he brag about funding?
What legislation does he introduce?
How often does he talk about Chesapeake, defense, Republicans, and other hot button topics?
Is there a pattern to when he issues releases-- example: typically Mondays, more around elections?
How often does he "applaud," "call for," "urge," and "honor" -- the buzzwords of politics?



```{r}
#cardin_url <- "https://www.cardin.senate.gov/?post_type=press-releases" 

#cardin <- cardin_url |>
 # read_html() |>
  #html_element('span.customBlog_container') |> 
  #html_text()

# show the dataframe customBlog_container

#cardin



library(rvest)
library(dplyr)


# Scrape the webpage
cardin <- read_html("https://www.cardin.senate.gov/?post_type=press-releases")

# Extract dates, titles, and URLs
dates <- cardin %>% html_elements("h5") %>% html_text()
titles <- cardin %>% html_elements("h3") %>% html_text()
urls <- cardin %>% html_elements("a") %>% html_attr("href")

# Convert lists to data frames
dates_df <- data.frame(date = dates)
titles_df <- data.frame(title = titles)
urls_df <- data.frame(url = urls)




```
```{r}
filtered_urls_df <- urls_df %>% 
  filter(str_detect(url, "^https://www.cardin.senate.gov/press-releases/")) |>
  distinct() |>
  

# View the resulting data frame
print(filtered_urls_df)
```

```{r}
filtered_urls_df <- filtered_urls_df[-1, ]
```

```{r}
# Combine into a single data frame
press_releases_df <- bind_cols(dates_df, titles_df, filtered_urls_df)

# View the resulting data frame
print(press_releases_df)
```

```{r}
cardin_releases <-press_releases_df |>
  clean_names() |>
  rename(link = x3)

```

```{r}
cardin_releases <- cardin_releases |>
  mutate(date = mdy(date))
```

